

# 结论

deepseek系列大模型各项性能达到了业内领先，并有更低的推理成本。从技术上没有革命性创新，但有较大程度的优化。此外，deepseekv3和r1系列的发布对于商业模式和意识形态有很大影响。

接下来从以下几个方面展开叙述

## 1.deepseek和GPT系列的比较

deepseek这次火出圈主要是v3和r1版本，v3是通用版本，r1是深度思考版本相对v3使用了完全的强化学习，因此需要更多算力。

根据官网的介绍，DeepSeek-V3 多项评测成绩超越了 Qwen2.5-72B 和 Llama-3.1-405B 等其他开源模型，并在性能上和世界顶尖的闭源模型 GPT-4o 以及 Claude-3.5-Sonnet 不分伯仲，此外**自称训练成本不到GPT-4o的1/20**

首先性能上接近，可信度很高，包括最新推出的GPT-4o3min系列，公布的跑分依旧和DeepSeek-V3，R1系列不相上下；

其次，成本确实低了很多。但具体的数值有点夸张，比如自称训练成本不到1/20，其实是没有算试错成本的；提供对外的API价格只有GPT-o1的3%，但这一部分只是deepseek价格战策略的原因，open ai为了应对，新出的o3-min对应也降价了。所以实际运营成本，**我个人估计deepseek在GPT的20%～50%**。

> DeepSeek-R1 API 服务定价为每百万输入token  4 元（缓存未命中），每百万输出 tokens 16 元
>
>  Deepseek v3每百万输入 tokens  1 元（缓存未命中），每百万输出 tokens 2 元
>
> o3-mini每百万token的输入（缓存未命中）1.10美元，输出价格为 4.40美元

## 2.deepseek对于开源闭源，意识形态，商业模式的影响

### 开源闭源

大模型一直有开源闭源的争论，开源的方面认为开源才能集思广益提供更多创造，闭源的认为大模型很烧钱，必须形成商业模式才能持续发展，在deepseek出现之前，闭源模型性能一直处于领先。

deepseek极大的缩小了开源和闭源模型之间的差距，open ai奥特曼指出，虽然OpenAI将在未来继续开发更先进的模型，但与DeepSeek相比，其当前的领先优势将大幅削弱。

开源模型一旦追上闭源模型，这对闭源模型的商业模式的冲击是巨大的（后面会提到），闭源模型想再次领先就困难了。

### 意识形态

长期以来，以美国为首的西方国家都在虹吸世界各处的人才，现在AI是科技实力的重要代表，deepseek后来居上这对西方国家的意识形态输出的打击是巨大的。

西方对中国模型本身并不放心，他觉得模型内部训练时就有了价值观偏见（当然我们对西方国家也有这样的顾虑）。因为生成类AI的输出风格和他的训练样本非常相关。大家都知道中国互联网基本处于一个封闭状态，西方人会对“模型自带的意识形态”产生害怕心理。而选择开源的话，就相当你把我模型的数据（不是训练模型的数据）下载下来放到自己的GPU上去跑，包括我们今天看到Amazon和英伟达的云服务器都支持了DeepSeek模型，就是因为它选择开源模式。部署到本机内部，西方也无所谓什么顾虑，不管什么价值观输出，至少不存在信息传回政府这种。

deepseek开源了，且足够优秀，那不可避免的会有很多国家和组织使用，西方国家长期宣传的“美国领先”也会收到更多的质疑。

### 商业模式

DeepSeek由金融机构幻方量化开发，能够在投研、量化交易、自动化交易等多个金融场景中进行自验证。做量化交易，和普通公司最大的不同，他们在“玩钱”，“玩杠杆”，是非常危险的事，从这种角度，做量化交易的人往往更脚踏实地，不会道听途说哪种算法好就拿来用，即使是Google，OpenAI发明的算法，它也会做非常非常多的修改。相反很多AI公司他们多是互联网所谓”大厂“思维 – 想着怎么用产品经理的思维把产品做大，然后去marketing，去PR。产品本身如何在其次，至少用广告打造、宣传让大家认为产品有前途。量化交易不是这种思维。通俗一点讲，目前各大互联网公司都在吹牛逼的技术，拿融资，所以各种刷榜发论文，其实忽略了模型本身的能力的打磨。当然deepseek发布后，各大模型又要洗牌，本身是一个开源模型大家肯定会在技术上查漏补缺立马跟上这是利好；deepseek打价格战，各个模型卖API接口的收益要大打折扣了，而且以后融资也也更难拿了，这是很大的冲击。

## 3.deepseek的技术创新点

选择性的看了deepseek官方的三篇论文，比较新颖的技术点还是mla和mtp，这两个技术显著的降低了训练和推理的成本

### 1）多头潜在注意力（MLA）——稀疏键值

传统的Transformer架构中的多头部注意力（MHA）的键值（KV）缓存对LLMs的推理效率构成了重大障碍，虽然有了分组查询注意力（GQA）和多查询注意力（MQA），但是这些方法在试图减少KV缓存时往往牺牲了性能

在传统的多头注意力机制中，对于每一个输入的token,我们需要在每一层的attention中存储它的键(key)和值(value)向量，以便后续的token可以通过查询(query)这些键值对来计算注意力权重

这些键值向量的总元素数量等于2nhdh，其中nh是注意力头的数量,dh是每个头的维度

当模型的层数、头数和维度较大时，这个缓存的开销非常大

与传统的注意力机制不同，MLA利用**低秩键值联合压缩方法**，在推理过程中通过将键值（KV）缓存压缩成潜在向量来大幅减少内存占用，从而提高推理效率；为了进一步提升性能，DeepSeek-V2对查询和键进行解耦，并为每个注意力头设置了独立的维度，以提高模型的表达能力

键值联合低秩压缩:将原本需要单独存储的键向量和值向量压缩到了一个维度更低的向量cKVt中,这个压缩后的向量维度为dc，远小于原来的2nhdh在推理时，我们只需要存储这个压缩后的cKVt向量，而不需要存储原始的键值向量



用通俗的一点话表达，就是把存储注意力头的信息的向量映射压缩到更低的维度了，所以存储空间小了训练速度快了。理论上这是会有一些损失的，类似于图片的有损压缩，比较低的损失换来很大的空间节省，但是本论文中损失没有具体表达，估计是不好计算。这个方法也不算完全原创吧，类似于lora微调都是利用这样的方法，不过在多注意力头这样用的，比较成功的大模型，deepseek确实是第一个，后面估计大家都会用了。

### 2）MTP多token预测

DeepSeek引入的多token预测(MTP)技术堪称一个Game Changer。这项技术实际上是Meta在2024年4月30号提出的，DeepSeek对新技术的应用甚至快过Meta自己。

语言模型一次只预测一个token的范式。它就像是让模型从"一字一句"地朗读，进化为"整句整段"地理解和生成。在训练过程中，模型不再局限于预测序列中的下一个token，而是学会同时预测多个连续位置的token。这种并行预测机制不仅提高了训练效率，还让模型能够更好地捕捉token之间的依赖关系。在保持输出质量的同时，模型整体性能提升2-3%。

### 3）DualPipe跨节点通信

DualPipe引入了双重流水线的概念，就像在同一条生产线上同时处理两批产品。当一个计算阶段在等待数据传输时，可以立即切换到处理另一批数据，这样就能充分利用原本的空闲时间。

涉及到的很多，就不一一介绍了。整体看起来deepseek其实没有太多技术创新，包括mtp，思维链，dual pipe，混合精度，SFT，RL的方法其实各个大模型都在用，并非deepseek原创。硬要说我觉得还是商业模式上的务实，很多技术都是走的远，比如mtp比提出它的meta走的更远。

## 4.deepseek与AGI的关系

前面其实也有提到，deepseek的贡献DeepSeek是站在前人0到1的基础上，比如思维链这种，他不是第一个提出来，但是它挖得比较深从1到了10，但deepseek本质上还是transformer系列基于数据的大模型，deepseek离AGI的距离和GPT离AGI的距离是一样的。

## 5.deepseek的部署和使用

虽然大模型的部署有一定门槛但没有研究的必要。

目前各大云平台都有镜像可以直接使用，需要本地使用也可以直接pull部署到本地（当然现在没卡，也没有这种需求）。直接使用就更简单了，开源模型使用基本都免费，直接官方网页端或者手机APP就行。



调查了一下**时空折叠技术**是实现 MTP（多token预测）的一种算法。上一篇报告中，我也解释了，我觉得MTP和MLA是deepseek能用更小的成本达到同等性能的关键。

> MTP改变了语言模型一次只预测一个token的范式。它就像是让模型从"一字一句"地朗读，进化为"整句整段"地理解和生成。在训练过程中，模型不再局限于预测序列中的下一个token，而是学会同时预测多个连续位置的token。这项技术实际上是Meta在2024年4月30号提出的，deepseek也用了这个思想，并改良了算法。

之前deepseek v3的论文发出来后，国外质疑deepseek抄袭，后面证明了**时空折叠技术**是原创

其实具体怎么做的论文中描述的并不是很清楚，我觉得正是因为这样别人才会质疑（毕竟是核心机密不会那么轻易告诉别人，目前deepseek参数开源了但是训练过程没开源）以下是deepseekv3论文中关于mtp算法的描述：

原文在 附件论文2.2 Multi-Token Prediction

> DeepSeek V3创新引入**残差流分形解码架构**（也就是网传的时空折叠技术）： 
>
> 1. **主预测模块**：输出当前token概率分布（标准模式） 
>
> 2. **次预测模块**：将最终残差流注入轻量化 Transformer 子块，生成次 token 预测 
>
> 3. **动态损失融合**：主次预测损失以 7:3 权重混合训练，兼顾精度与前瞻性 
>
>    论文中的结论：该设计使单次前向传播学习效率提升 1.8 倍，在代码补全任务中，token 预测准确率相对位置误差降低 42%。
>

单从这些看起来确实有抄袭嫌疑，后面应该是提供了有力的证据证明了算法的创新，但是这个证明目前还没披露。
